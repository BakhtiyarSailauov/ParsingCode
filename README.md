'''
   Софт, который будет парсить сайты определенного калибра.
<<<<<<< HEAD
   ПС: Определенный, так как во время пробных запусков кода, понял что не все 
   сайты позволяют использовать себя как полигон для экспериментов 
'''
import requests
#библеотека --requests--, предназначена, 
#что бы вызывать HTML код в нашем софте

from bs4 import BeautifulSoup
#--bs4--, чтобы упорядчить и организовать 
#поиск в HTML коде

from time import sleep 
#базовая библеотека --time-- чтобы организовать
#таймер на запросы через requests, во избежание исключений 

headers = {}
#задаем в ручную источник запроса HTML-кода, во избежание блокировки источники

def download(url):
    #роль функций скачывать любого типа данных, с помощью URL адреса
    resp = requests.get(url, stream = True)
    #--stream-- это для того чтобы регулировать способ скачивание
    r = open("C:\\Users\\Pavilion\\Desktop\\Python\\parser\\image\\" + url.split("/")[-1], "wb")
    #скачиваем с помощью перезаисывание существуещего файла
    for value in resp.iter_content(1024*1024):   #1024*1024 обьем допускаемого данных
        r.write(value)
    r.close()

def get_url():
    for page in range(1, 8):
        #во время поиска, обычно не будет единичных страницы с катологами.
        #с помочью этого цикла реализуем переход между старницами

        url = f'https://scrapingclub.com/exercise/list_basic/?page={page}'
        requests_data = requests.get(url, headers = headers)
        #вызываем HTML код, передавая адрес сайта на -requests.get-

        soup = BeautifulSoup(requests_data.text, 'lxml')
        #--lxml-- инструмент, сортирующий HTML-код по порядку
        #и дальше передает на bs4 

        array_data = soup.find_all("div", class_ = "col-lg-4 col-md-6 mb-4")
        #собираем все катологи на массив

        for data in array_data:
            #обход всех катологов

            link_page = "https://scrapingclub.com" + data.find('a').get('href')
            yield link_page
            #собираем ссылки на отдельные старницы в массив

def get_data_array():
    #результаты вернем с помощью генератора, 
    #чтобы в Ехсел модуле сохранять все на таблицах 
    for page_url in get_url():
        #обход каждого страницу будет отдельно для наглядности. 
        #ПС: писать код таким образом - ХАРАМ.
        
        
второй модуль input_modul: - #модуль создан, что-бы данные взятые из парсинга сохранять в Exsel таблицу
=======
   PS: Определенный, так как во время пробных запусков кода, понял, что не все
   сайты позволяют использовать себя как полигон для экспериментов
'''
запросы на импорт
# библеотека --requests--, совокупность,
# что бы отображать HTML-код в нашем софте

из bs4 импортировать BeautifulSoup
# --bs4--, чтобы упорядочить и привлечь
# поиск в HTML коде

из времени импортировать сон
# базовая библеотека --time-- чтобы путешествовать
# таймер на запросы через запросы, во избежание исключений

заголовки = {}
# задаем в ручную источник запроса HTML-кода, пропуская исходников

загрузка деф (url):
    #рольфункции скачивать данные любого типа по URL-адресу
    resp = request.get (url, поток = True)
    #--stream-- это для того, чтобы регулировать способ скачивания
    r = open("C: \\ Users \\ Pavilion \ Desktop \\ Python \\ parser \ image \ " + url.split("/")[-1], "wb")
    #скачиваем с помощью перезапуска существующего файла
    для значений в resp.iter_content(1024 *1024): #1024* 1024 обьем допустимого данных
        r.write(значение)
    р.закрыть()

защита get_url():
    для страниц в сознании (1, 8):
        #во время поиска, обычно не будет единичных страниц с катологами.
        #с помощью этого цикла реализуем переход между старницами

        url = f'https://scrapingclub.com/exercise/list_basic/?page={page}'
        request_data = request.get (url, заголовки = заголовки)
        #вызываем HTML код, передаем адрес сайта на -requests.get-

        суп = BeautifulSoup(requests_data.text, 'lxml')
        #--lxml-- инструмент, сортирующий HTML-код по порядку
        #и дальше передать на bs4

        array_data = суп.find_all («div», class_ = «col-lg-4 col-md-6 mb-4»)
        #собираем все катологи на массив

        для данных в array_data:
            #обход всех катологов

            link_page = "https://scrapingclub.com" + data.find('a').get('href')
            выход link_page
            #собираем ссылки на отдельные старницы в массиве

защита get_data_array():
    #результаты вернем с помощью генератора,
    #чтобы в Ехсел модуль читается все в таблицах
    для page_url в get_url():
        #обход каждой страницы будет отдельно для наглядности.
        #ПС: писать код таким образом - ХАРАМ.


второй модуль input_modul: - #modul created, что-бы приложения применялись из парсинга ресурсов в таблице Exsel
>>>>>>> 0f031d4 (Ignore unnecessary files)
